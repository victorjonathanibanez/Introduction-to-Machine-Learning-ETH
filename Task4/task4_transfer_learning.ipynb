{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decreased-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import pathlib\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "turkish-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import (cross_val_score, KFold, train_test_split)\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def extract_image_features(image_directory, cnn_model):\n",
    "    image_features = {}\n",
    "    # collect all files from input directory\n",
    "    count = 1\n",
    "    for root, dirs, files in os.walk(image_directory):\n",
    "        for name in files:\n",
    "            if not name[-4:] == '.jpg': #Â skip non jpg files\n",
    "                continue\n",
    "            if count % 1000 == 0: \n",
    "                print('1k')\n",
    "            img_path = os.path.join(root, name)\n",
    "            img_id = name[:-4]\n",
    "            img = image.load_img(img_path, target_size=(299, 299))\n",
    "            img_data = image.img_to_array(img)\n",
    "            img_data = np.expand_dims(img_data, axis=0)\n",
    "            img_data = preprocess_input(img_data)\n",
    "\n",
    "            cnn_feature = cnn_model.predict(img_data)\n",
    "\n",
    "            image_features[img_id] = cnn_feature\n",
    "            count += 1\n",
    "    return image_features\n",
    "\n",
    "\n",
    "def train_split(n_splits, test_size, training_triplets):\n",
    "    \n",
    "    collect_train_splits = []\n",
    "    collect_test_splits = []\n",
    "    \n",
    "    random.seed(42)\n",
    "    seeds = random.sample(range(100), n_splits)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        random.seed(seeds[i])\n",
    "\n",
    "        # generate random image ids\n",
    "        test_ids = random.sample(range(5000), 5)\n",
    "\n",
    "        # select all triplets containing at least one of the image ids\n",
    "        test_index = training_triplets.index[training_triplets.isin(test_ids).any(1)]\n",
    "\n",
    "        # add one new id and collect all triplets containing it\n",
    "        # continue until test set size is reached\n",
    "        while len(test_index) <= test_size:\n",
    "            test_index = training_triplets.index[training_triplets.isin(test_ids).any(1)]\n",
    "            new_ids = np.unique(training_triplets.iloc[test_index].to_numpy()).tolist()\n",
    "\n",
    "            n_test_ids = len(test_ids)\n",
    "            while n_test_ids == len(test_ids):\n",
    "                new_id = random.choice(new_ids)\n",
    "\n",
    "                if new_id in test_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    test_ids.append(new_id)\n",
    "\n",
    "        # get training triplets as all triplets that don't contain any of the test ids\n",
    "        all_test_ids = np.unique(training_triplets.iloc[test_index].to_numpy()).tolist()\n",
    "        train_index = training_triplets.index[~training_triplets.isin(all_test_ids).any(1)]\n",
    "        \n",
    "        # sort and convert to list to read from hdf5 file\n",
    "        train_index = np.concatenate((train_index, train_index+len(training_triplets)))\n",
    "        test_index = np.concatenate((test_index, test_index+len(training_triplets)))\n",
    "        train_index.sort()\n",
    "        test_index.sort()\n",
    "        train_index = list(train_index)\n",
    "        test_index = list(test_index)\n",
    "        \n",
    "        # gather splits\n",
    "        collect_train_splits.append(train_index)\n",
    "        collect_test_splits.append(test_index)\n",
    "        \n",
    "        print('k-fold ', i)\n",
    "        print('Number of training triplets:', len(train_index))\n",
    "        print('Number of testing triplets:', len(test_index))\n",
    "        \n",
    "    return collect_train_splits, collect_test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "square-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59515, 3)\n",
      "(59544, 3)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "image_dir = './food'\n",
    "train_triplets = pd.read_csv('train_triplets.txt', sep=' ', header=None)\n",
    "test_triplets = pd.read_csv('test_triplets.txt', sep=' ', header=None)\n",
    "train_triplets.columns = ['A', 'B', 'C']\n",
    "test_triplets.columns = train_triplets.columns\n",
    "print(train_triplets.shape)\n",
    "print(test_triplets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "freelance-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained inceptionv3 model for feature extraction\n",
    "vision_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infinite-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n"
     ]
    }
   ],
   "source": [
    "# testrun feature extraction\n",
    "sample_img = './food/00000.jpg'\n",
    "img = image.load_img(sample_img, target_size=(299, 299))\n",
    "img_data = image.img_to_array(img)\n",
    "img_data = np.expand_dims(img_data, axis=0)\n",
    "img_data = preprocess_input(img_data)\n",
    "\n",
    "sample_feature = vision_model.predict(img_data)\n",
    "print(sample_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image feature dictionary\n",
    "inceptionv3_features_dict = extract_image_features(image_dir, vision_model)\n",
    "\n",
    "# save image features\n",
    "np.save('inceptionv3_features_dict.npy', inceptionv3_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary\n",
    "image_features = np.load('inceptionv3_features_dict.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suburban-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all training samples:  119030\n",
      "Number of submission samples:  59544\n",
      "Number of triplet features:  6144\n"
     ]
    }
   ],
   "source": [
    "n_images = len(image_features)\n",
    "\n",
    "# read all image feature to array\n",
    "raw_features = np.zeros(shape=(n_images, 2048), dtype='float32')\n",
    "for i in range(n_images):\n",
    "    raw_features[i] = image_features['{:0>5d}'.format(i)]\n",
    "\n",
    "n_train_samples = 2 * train_triplets.shape[0]\n",
    "n_test_samples = test_triplets.shape[0]\n",
    "n_feature_maps = 3 * raw_features.shape[-1]\n",
    "print('Number of all training samples: ', n_train_samples)\n",
    "print('Number of submission samples: ', n_test_samples)\n",
    "print('Number of triplet features: ', n_feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and submission data and save on disc\n",
    "with h5py.File('task4_data.hdf5','w') as f:\n",
    "    X = f.create_dataset('X', (n_train_samples, n_feature_maps), dtype='float32')\n",
    "    X_submission = f.create_dataset('X_submission', (n_test_samples, n_feature_maps), dtype='float32')\n",
    "    \n",
    "    # create class 1 training samples\n",
    "    for i in range(int(len(X)/2)):\n",
    "        imgA_features = raw_features[train_triplets['A'][i]]\n",
    "        imgB_features = raw_features[train_triplets['B'][i]]\n",
    "        imgC_features = raw_features[train_triplets['C'][i]]\n",
    "        features = np.concatenate((imgA_features, imgB_features, imgC_features), axis=-1)\n",
    "        X[i] = features\n",
    "        \n",
    "    # create class 0 training samples\n",
    "    for i in range(int(len(X)/2)):\n",
    "        imgA_features = raw_features[train_triplets['A'][i]]\n",
    "        imgB_features = raw_features[train_triplets['B'][i]]\n",
    "        imgC_features = raw_features[train_triplets['C'][i]]\n",
    "        features = np.concatenate((imgA_features, imgC_features, imgB_features), axis=-1)\n",
    "        X[i+int(len(X)/2)] = features\n",
    "        \n",
    "    # create submission samples\n",
    "    for i in range(len(X_submission)):\n",
    "        imgA_features = raw_features[test_triplets['A'][i]]\n",
    "        imgB_features = raw_features[test_triplets['B'][i]]\n",
    "        imgC_features = raw_features[test_triplets['C'][i]]\n",
    "        features = np.concatenate((imgA_features, imgB_features, imgC_features), axis=-1)\n",
    "        X_submission[i] = features\n",
    "        \n",
    "    print(X.shape, X.dtype)\n",
    "    print(X_submission.shape, X_submission.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noticed-usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119030,)\n"
     ]
    }
   ],
   "source": [
    "# create labels\n",
    "y = np.zeros(shape=(n_train_samples))\n",
    "y[0:int(n_train_samples/2)] += 1\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "processed-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tensorflow session\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "european-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifier model\n",
    "inputs = tf.keras.Input(shape=(n_feature_maps))\n",
    "x = tf.keras.layers.Activation('relu')(inputs)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.Dense(1024)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = tf.keras.layers.Dense(256)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = tf.keras.layers.Dense(64)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = tf.keras.layers.Dense(8)(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "outputs = tf.keras.layers.Activation('sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "electronic-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.save_weights('initial_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conventional-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training variables\n",
    "epochs = 11\n",
    "batch_size = 32\n",
    "kfolds = 2\n",
    "validation_size = 2000\n",
    "early = EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acquired-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold  0\n",
      "Number of training triplets: 26494\n",
      "Number of testing triplets: 4016\n",
      "k-fold  1\n",
      "Number of training triplets: 26728\n",
      "Number of testing triplets: 4014\n"
     ]
    }
   ],
   "source": [
    "# get training and validation split\n",
    "X_train_inds, X_test_inds = train_split(kfolds, validation_size, train_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hired-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.70\n",
      "Initial accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('initial_weights.h5')\n",
    "\n",
    "# testrun model\n",
    "with h5py.File('task4_data.hdf5','r') as f:\n",
    "    loss0, accuracy0 = model.evaluate(x=f['X'][X_train_inds[0]], y=y[X_train_inds[0]],\n",
    "                                      batch_size=batch_size,\n",
    "                                      verbose=0)\n",
    "    print('Initial loss: {:.2f}'.format(loss0))\n",
    "    print('Initial accuracy: {:.2f}'.format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "heavy-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26494 samples, validate on 4016 samples\n",
      "Epoch 1/11\n",
      "26494/26494 [==============================] - 97s 4ms/sample - loss: 0.6932 - accuracy: 0.4934 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/11\n",
      "26494/26494 [==============================] - 98s 4ms/sample - loss: 0.6932 - accuracy: 0.4983 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/11\n",
      "26494/26494 [==============================] - 98s 4ms/sample - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/11\n",
      "26494/26494 [==============================] - 93s 3ms/sample - loss: 0.6649 - accuracy: 0.5879 - val_loss: 0.5967 - val_accuracy: 0.6765\n",
      "Epoch 5/11\n",
      "26494/26494 [==============================] - 107s 4ms/sample - loss: 0.5758 - accuracy: 0.6917 - val_loss: 0.5840 - val_accuracy: 0.6917\n",
      "Epoch 6/11\n",
      "26494/26494 [==============================] - 92s 3ms/sample - loss: 0.5520 - accuracy: 0.7102 - val_loss: 0.6021 - val_accuracy: 0.6830\n",
      "Epoch 7/11\n",
      "26494/26494 [==============================] - 92s 3ms/sample - loss: 0.5315 - accuracy: 0.7250 - val_loss: 0.5949 - val_accuracy: 0.6860\n",
      "Epoch 8/11\n",
      "26494/26494 [==============================] - 95s 4ms/sample - loss: 0.5108 - accuracy: 0.7424 - val_loss: 0.5878 - val_accuracy: 0.6880\n",
      "Epoch 9/11\n",
      "26494/26494 [==============================] - 98s 4ms/sample - loss: 0.4931 - accuracy: 0.7521 - val_loss: 0.5979 - val_accuracy: 0.6830\n",
      "Epoch 00009: early stopping\n",
      "Train on 26728 samples, validate on 4014 samples\n",
      "Epoch 1/11\n",
      "26728/26728 [==============================] - 92s 3ms/sample - loss: 0.6617 - accuracy: 0.5934 - val_loss: 0.6079 - val_accuracy: 0.6654\n",
      "Epoch 2/11\n",
      "26728/26728 [==============================] - 96s 4ms/sample - loss: 0.5661 - accuracy: 0.6980 - val_loss: 0.5744 - val_accuracy: 0.6903\n",
      "Epoch 3/11\n",
      "26728/26728 [==============================] - 92s 3ms/sample - loss: 0.5363 - accuracy: 0.7247 - val_loss: 0.5849 - val_accuracy: 0.6856\n",
      "Epoch 4/11\n",
      "26728/26728 [==============================] - 92s 3ms/sample - loss: 0.5188 - accuracy: 0.7362 - val_loss: 0.5786 - val_accuracy: 0.6963\n",
      "Epoch 5/11\n",
      "26728/26728 [==============================] - 94s 4ms/sample - loss: 0.4943 - accuracy: 0.7541 - val_loss: 0.5965 - val_accuracy: 0.6809\n",
      "Epoch 6/11\n",
      "26728/26728 [==============================] - 93s 3ms/sample - loss: 0.4789 - accuracy: 0.7638 - val_loss: 0.5916 - val_accuracy: 0.6913\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "# normal training with early stopping / cross validation if kfolds > 1\n",
    "for i in range(kfolds):\n",
    "    model.load_weights('initial_weights.h5')\n",
    "    \n",
    "    with h5py.File('task4_data.hdf5','r') as f:\n",
    "        history = model.fit(x=f['X'][X_train_inds[i]], y=y[X_train_inds[i]],\n",
    "                            batch_size=32,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[early],\n",
    "                            validation_data=(f['X'][X_test_inds[i]], y[X_test_inds[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load initial weights before training full model\n",
    "model.load_weights('initial_weights.h5')\n",
    "\n",
    "best_epochs = 7\n",
    "\n",
    "# train full model\n",
    "with h5py.File('task4_data.hdf5','r') as f:\n",
    "    print('Training..')\n",
    "    model.fit(x=f['X'][:], y=y, epochs=best_epochs, batch_size=32)\n",
    "    \n",
    "    # predict on submission file\n",
    "    prediction = model.predict(f['X_submission'][:])\n",
    "    print('..done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "prediction = np.where(prediction < 0.5, 0, 1)\n",
    "np.savetxt('submission.txt', prediction, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-treasure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlintro",
   "language": "python",
   "name": "mlintro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
